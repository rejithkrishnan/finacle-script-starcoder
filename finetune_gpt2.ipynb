{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 1: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rejit\\Development\\ai\\finacle-gpt2-finetune\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      5\u001b[39m     GPT2Tokenizer,\n\u001b[32m      6\u001b[39m     GPT2LMHeadModel,\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m     DataCollatorForLanguageModeling,\n\u001b[32m     10\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 2: Dataset Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinacleScriptDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.examples = []\n",
    "        with open(file_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        for item in data:\n",
    "            text = f\"PROMPT: {item['prompt']}\\nSCRIPT: {item['script']}{self.tokenizer.eos_token}\"\n",
    "            tokenized_text = self.tokenizer(text, truncation=True, max_length=128)\n",
    "            self.examples.append(tokenized_text)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.examples[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 3: Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading model and tokenizer...\")\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 4: Prepare Dataset and Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preparing dataset...\")\n",
    "train_file_path = \"training_data.json\"\n",
    "train_dataset = FinacleScriptDataset(file_path=train_file_path, tokenizer=tokenizer)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 5: Define Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine-tuned-model\",\n",
    "    num_train_epochs=30,\n",
    "    per_device_train_batch_size=1,\n",
    "    learning_rate=5e-6,\n",
    "    warmup_steps=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=15,\n",
    "    dataloader_pin_memory=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 6: Initialize and Run the Trainer\n",
    "\n",
    "This is the cell that will perform the training. It will take a few moments to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=None,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Starting the fine-tuning process...\")\n",
    "trainer.train()\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 7: Save the Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving the final model...\")\n",
    "trainer.save_model(\"./fine-tuned-model\")\n",
    "print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 8: Analyze and Plot Training Loss ðŸ“ˆ\n",
    "\n",
    "This final cell will parse the training logs and generate a plot to visualize how the model's loss decreased over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training history is stored in trainer.state.log_history\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "# Filter out the final summary metric to only get per-step logs\n",
    "training_logs = [log for log in log_history if 'loss' in log]\n",
    "\n",
    "# Create a pandas DataFrame for easier plotting\n",
    "df = pd.DataFrame(training_logs)\n",
    "\n",
    "# Create the plot\n",
    "plt.style.use('seaborn-v0_8-whitegrid') # Use a nice style for the plot\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(df['epoch'], df['loss'], marker='o', linestyle='-', color='b', label='Training Loss')\n",
    "\n",
    "# Add titles and labels for clarity\n",
    "plt.title('Training Loss Over Epochs', fontsize=16)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.xticks(range(0, int(df['epoch'].max()) + 2)) # Ensure integer ticks for epochs\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 9: Load the Fine-Tuned Model\n",
    "\n",
    "This cell loads the model and tokenizer that you saved in the previous step. You only need to run this once after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load the fine-tuned model and tokenizer from disk ---\n",
    "\n",
    "print(\"Loading the fine-tuned model...\")\n",
    "model_path = './fine-tuned-model'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "# If you are using a GPU, move the model to the GPU\n",
    "if torch.cuda.is_available():\n",
    "    model.to('cuda')\n",
    "\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 10: Define a Reusable Generation Function\n",
    "This cell defines a handy function to generate a script from any prompt. This makes it easy to test many different prompts without rewriting code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_finacle_script(prompt_text):\n",
    "    \"\"\"\n",
    "    This function takes a text prompt and generates a Finacle script\n",
    "    using the loaded fine-tuned model.\n",
    "    \"\"\"\n",
    "    # Format the prompt exactly as we did during training\n",
    "    formatted_prompt = f\"PROMPT: {prompt_text}\\nSCRIPT:\"\n",
    "\n",
    "    # Tokenize the formatted prompt and move tensors to the correct device (GPU/CPU)\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors='pt')\n",
    "    input_ids = inputs.input_ids.to(model.device)\n",
    "    attention_mask = inputs.attention_mask.to(model.device)\n",
    "    \n",
    "    print(f\"\\n---> Generating script for prompt: '{prompt_text}'\")\n",
    "    \n",
    "    # Use the model's generate() method with sampling for more creative results\n",
    "    # Note: We are not using beam search (num_beams) here to allow temperature to work.\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=150,\n",
    "        do_sample=True,          # Activate sampling\n",
    "        temperature=0.7,         # Make output less random\n",
    "        top_k=50,                # Consider only the top 50 most likely tokens\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode the generated token IDs back into text\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Clean up the output to only show the generated script\n",
    "    try:\n",
    "        script_part = generated_text.split(\"SCRIPT:\")[1].strip()\n",
    "    except IndexError:\n",
    "        script_part = \"Could not generate a valid script.\"\n",
    "        \n",
    "    # Print the result in a formatted way\n",
    "    print(\"\\n--- Generated Finacle Script ---\")\n",
    "    print(script_part)\n",
    "    print(\"--------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 11: Test the Generation Function\n",
    "Now you can use the function from the previous cell to test any prompt you like. Simply call the function with your desired prompt as the argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example 1 ---\n",
    "generate_finacle_script(\"Write a script that subtracts 5 from 20 and prints the result.\")\n",
    "\n",
    "# --- Example 2 ---\n",
    "generate_finacle_script(\"Create a while loop that prints numbers from 1 to 3.\")\n",
    "\n",
    "# --- Example 3 (You can add your own) ---\n",
    "generate_finacle_script(\"Convert the string 'FINACLE' to lowercase.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
